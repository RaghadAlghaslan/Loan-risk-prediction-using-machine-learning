import pandas as pd
import numpy as np
import sklearn
from sklearn import preprocessing
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split 
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from imblearn.under_sampling import RandomUnderSampler
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score


df = pd.read_csv('Training Data.csv')


## Model training
##### since the class label is imbalanced and the dataset is large, then some of the suitable algorithems are decision tree (as a baseline, can handle imbalance data because they make splits based on the information gain) and Random Forest (can handle class imbalance by using bootstrap aggregating (bagging)) which are going to be the models in subject.


# viewing the data set after the preprocessing
df.head()


# Export the DataFrame to a CSV file for further use
df.to_csv('output.csv')


# Encoding Categorical Data into different data frame to be able to scale models
encoder = LabelEncoder()
categorical = ['Married/Single','House_Ownership','Car_Ownership','Profession', 'Income Range']
for cols in categorical:
    df[cols] = encoder.fit_transform(df[cols])


# Setting the X (features) and Y (label/output/target)
X = df.drop(['Risk_Flag'], axis= 1)
Y = df.Risk_Flag


### Scaling the features using MinMaxScaler 
##### since most of the features has a uniform ditribution
mm_scaler = preprocessing.MinMaxScaler()
X_mm = mm_scaler.fit_transform(X)
# Split the data into training and testing sets 
X_train, X_test, y_train, y_test = train_test_split(X_mm, Y, test_size = 0.4, random_state=42, stratify=Y) 


### Decision tree
# Initialize the decision tree classifier
dct = DecisionTreeClassifier(random_state=42)
# Fit the classifier to the training data
dct.fit(X_train, y_train)
# Make predictions on the testing data
dtc_y_pred = dct.predict(X_test)
print('Training Accuracy: {:.3f}'.format(dct.score(X_train, y_train)))
print('Test Accuracy: {:.3f}'.format(dct.score(X_test, y_test)))


### Random Forest
# Initialize the random forest classifier with class weights
RF_model = RandomForestClassifier(n_estimators=100, class_weight='balanced')
# Train the model on the training data
RF_model.fit(X_train, y_train)
# Make predictions for test data
RF_y_pred = RF_model.predict(X_test)
print('Training Accuracy : {:.3f}'.format(RF_model.score(X_train, y_train)))
print('Test Accuracy : {:.3f}'.format(RF_model.score(X_test, y_test)))


### Using undersampling 
##### since we have imbalance class labels and a very large dataset
# Initialize the random under sampler
rus = RandomUnderSampler(random_state=42)
# Resample the training data
X_train_res, y_train_res = rus.fit_resample(X_train, y_train)
### Decision tree with undersampled dataset
# Fit the classifier to the undersampled training data
dct.fit(X_train_res, y_train_res)
# Make predictions on the testing data
dtc_y_pred = dct.predict(X_test)
print('Training Accuracy: {:.3f}'.format(dct.score(X_train_res, y_train_res)))
print('Test Accuracy: {:.3f}'.format(dct.score(X_test, y_test)))


##### as noticed the model is overfitting , use cross-validation to try to fix the issue.
# Perform 10-fold cross-validation with undersampling
scores = cross_val_score(dct, X_train_res, y_train_res, cv=10, scoring='accuracy')
# Print the cross-validation scores
print(f"Cross-validation scores: {scores}")
print(f"Mean score: {scores.mean():.3f}")


### Random forest with undersampled dataset
# Train the model on the undersampled training data
RF_model.fit(X_train_res, y_train_res)
# Make predictions for test data
RF_y_pred = RF_model.predict(X_test)
print('Training Accuracy : {:.3f}'.format(RF_model.score(X_train_res, y_train_res)))
print('Test Accuracy : {:.3f}'.format(RF_model.score(X_test, y_test)))


# Perform 7-fold cross-validation with undersampling
scores = cross_val_score(RF_model, X_train_res, y_train_res, cv=7, scoring='accuracy')
# Print the cross-validation scores
print(f"Cross-validation scores: {scores}")
print(f"Mean score: {scores.mean():.3f}")


### Oversampling
##### it helps with imbalance data
from imblearn.over_sampling import SMOTE
# Instantiate the SMOTE algorithm
smote = SMOTE(random_state=42)
# Fit and apply SMOTE to the data
X_train_ovsam, y_train_ovsam = smote.fit_resample(X_train, y_train)
# Print the number of samples in each class before and after oversampling
print("Before oversampling:\n", Y.value_counts())
print("After oversampling:\n", y_train_ovsam.value_counts())
print("Shape of X_train_ovsam:", X_train_ovsam.shape)


### Decision tree on oversampled data
# Initialize the decision tree classifier
dct = DecisionTreeClassifier(random_state=42)
# Fit the classifier to the training data
dct.fit(X_train_ovsam, y_train_ovsam)
# Make predictions on the testing data
dtc_y_pred = dct.predict(X_test)
print('Training Accuracy: {:.3f}'.format(dct.score(X_train_ovsam, y_train_ovsam)))
print('Test Accuracy: {:.3f}'.format(dct.score(X_test, y_test)))


### Random forest on oversampled data
# Initialize the random forest classifier without class weights since the data is now balanced
RF_model = RandomForestClassifier(n_estimators=100)
# Train the model on the training data
RF_model.fit(X_train_ovsam, y_train_ovsam)
# Make predictions for test data
RF_y_pred = RF_model.predict(X_test)
print('Training Accuracy : {:.3f}'.format(RF_model.score(X_train_ovsam, y_train_ovsam)))
print('Test Accuracy : {:.3f}'.format(RF_model.score(X_test, y_test)))
